---

  - hosts: localhost

    tasks:

      - include_vars: /opt/lab/environment.yml
        tags: always

      - set_fact:
          cns_namespace: "{{ CNS_NAMESPACE }}"
          cns_storageclass: "{{ CNS_STORAGECLASS }}"
          new_cns_nodes_internal_fqdn:
            - "{{NODE4_INTERNAL_FQDN}}"
            - "{{NODE5_INTERNAL_FQDN}}"
            - "{{NODE6_INTERNAL_FQDN}}"
          heketi_resturl: "http://heketi-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
          heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
          cns_storageclass2: "{{CNS_STORAGECLASS2}}"
          ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
          node_brick_device: "{{ NODE_BRICK_DEVICE }}"
          node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
          node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"
        tags: always

      - name: login as fancyuser1
        shell: oc login -u fancyuser1 -p openshift
        changed_when: false

      - name: ensure fancyuser1 can log in to project 'my-database-app'
        shell: oc project my-database-app
        changed_when: false

      - name: check pvc called 'postgres' exists
        shell: oc get pvc/postgresql -o json
        register: get_postgresql_pvc
        changed_when: false

      - set_fact:
          postgresql_pvc: "{{ get_postgresql_pvc.stdout|from_json }}"

      - name: ensure pvc is bound
        fail:
          msg: pvc is not bound
        when: postgresql_pvc.status.phase != "Bound"

      - name: ensure pvc is issued against CNS
        fail:
          msg: pvc is not satisfied from StorageClass {{ cns_storageclass }}
        when: postgresql_pvc.metadata.annotations['volume.beta.kubernetes.io/storage-class'] != cns_storageclass

      - shell: oc login -u system:admin -n my-database-app
        changed_when: false

      - name: check pv is exists
        shell: oc get pv/{{ postgresql_pvc.spec.volumeName }} -o json
        register: get_postgresql_pv
        changed_when: false

      - set_fact:
          postgresql_pv: "{{ get_postgresql_pv.stdout|from_json }}"

      - name: check pv is provided by CNS
        fail:
          msg: pv is not provided by {{ cns_storageclass }}
        when: postgresql_pv.metadata.annotations['volume.beta.kubernetes.io/storage-class'] != cns_storageclass

      - name: get storage class
        shell: oc get storageclass/{{ cns_storageclass }} -o json
        register: get_storageclass1
        changed_when: false

      - set_fact:
          storageclass1: "{{ get_storageclass1.stdout|from_json }}"

      - name: get CNS cluster id of storageclass
        set_fact:
          clusterid1: "{{storageclass1.parameters.clusterid}}"

      - name: get nodes from heketi using CNS cluster id
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} cluster info {{ clusterid1 }} --json
        register: get_cluster1_nodes
        changed_when: false

      - set_fact:
          cluster1_nodes: "{{ get_cluster1_nodes.stdout|from_json }}"

      - name: get id of first node of CNS cluster
        set_fact:
          cluster1_node1_id: "{{ cluster1_nodes.nodes|first }}"

      - name: get details of first node of CNS cluster
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ cluster1_node1_id }} --json
        register: get_cluster1_node1
        changed_when: false

      - set_fact:
          cluster1_node1: "{{ get_cluster1_node1.stdout|from_json }}"

      - set_fact:
          cluster1_node1_hostip: "{{ cluster1_node1.hostnames.storage|first }}"

      - name: get pod name of first node of cns cluster
        shell: oc get pods -o wide -n container-native-storage -o jsonpath='{$.items[?(@.status.hostIP=="{{ cluster1_node1_hostip }}")].metadata.name}'
        register: get_cluster1_node1_pod
        changed_when: false

      - set_fact:
          cluster1_node1_pod: "{{ get_cluster1_node1_pod.stdout }}"

      - set_fact:
          pv_gluster_vol_name: "{{ postgresql_pv.spec.glusterfs.path }}"

      - name: switch to cns namespace
        shell: oc project {{ cns_namespace }}
        changed_when: false

      - name: check gluster volume of pv
        shell: oc rsh {{ cluster1_node1_pod }} gluster vol info {{ pv_gluster_vol_name }}
        changed_when: false

      - name: log back in as fancyuser1 to my-database-app
        shell: oc login -u fancyuser1 -p openshift -n my-database-app
        changed_when: false

      - name: check health of postgres-db deployment
        shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.name=="postgresql")].status.readyReplicas}'
        register: pgsql_db_rc_check
        failed_when: pgsql_db_rc_check.stdout != "1"
        changed_when: false

      - name: check health of rails-app deployment
        shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.name=="rails-pgsql-persistent")].status.readyReplicas}'
        register: rails_app_rc_check
        failed_when: rails_app_rc_check.stdout != "1"
        changed_when: false

      - name: check if user fancyuser1 can log in to 'my-shared-storage'
        shell: oc project my-shared-storage
        changed_when: false

      - name: check health of file-uploader deployment
        shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="file-uploader")].status.readyReplicas}'
        register: fileuploader_rc_check
        failed_when: fileuploader_rc_check.stdout != "3"
        changed_when: false

      - name: check route of file-uploader sevice
        shell: oc get route/file-uploader
        changed_when: false

      - name: ensure rwx pvc exists
        shell: oc get pvc/my-shared-storage -o json
        register: get_file_uploader_pvc
        changed_when: false

      - set_fact:
          file_uploader_pvc: "{{ get_file_uploader_pvc.stdout|from_json }}"

      - name: ensure pvc is bound
        fail:
          msg: pvc is not bound
        when: file_uploader_pvc.status.phase != "Bound"

      - name: ensure pvc is issued against CNS
        fail:
          msg: pvc is not satisfied from StorageClass {{ cns_storageclass }}
        when: file_uploader_pvc.metadata.annotations['volume.beta.kubernetes.io/storage-class'] != cns_storageclass

      - shell: oc login -u system:admin -n my-database-app
        changed_when: false

      - name: check pv is exists
        shell: oc get pv/{{ file_uploader_pvc.spec.volumeName }} -o json
        register: get_file_uploader_pv
        changed_when: false

      - set_fact:
          postgresql_pv: "{{ get_file_uploader_pv.stdout|from_json }}"

      - name: check pv is provided by CNS
        fail:
          msg: pv is not provided by {{ cns_storageclass }}
        when: postgresql_pv.metadata.annotations['volume.beta.kubernetes.io/storage-class'] != cns_storageclass

      - name: log back in as fancyuser1 to my-shared-storage namespace
        shell: oc login -u fancyuser1 -p openshift -n my-shared-storage
        changed_when: false

      - name: ensure pv is file-uploader deployment config contains pvc
        shell: oc get dc/file-uploader -o jsonpath='{$.spec.template.spec.volumes[0].persistentVolumeClaim.claimName}'
        register: get_file_uploader_pvc_name
        failed_when: get_file_uploader_pvc_name.stdout != file_uploader_pvc.metadata.name
        changed_when: false

      - name: log back into cns namespace as cluster admin
        shell: oc login -u system:admin -n {{ cns_namespace }}
        changed_when: false

      - name: ensure additional CNS nodes are configured in the inventory
        fail:
          msg: node {{ item }} is not part of the cns group in the inventory
        when: item not in groups['cns']
        with_items: "{{ new_cns_nodes_internal_fqdn }}"

      - set_fact:
          iptables_rules:
            - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT"
            - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT"
            - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT"
            - "-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT"

      - name: ensure firewall rules are present on CNS nodes
        shell: iptables -w -S OS_FIREWALL_ALLOW
        register: iptables_rules
        failed_when: item[0] not in iptables_rules.stdout
        delegate_to: "{{ item[1] }}"
        with_nested:
          - "{{ iptables_rules }}"
          - "{{ new_cns_nodes_internal_fqdn }}"
        changed_when: False
        become: true

      - name: ensure additional storage nodes have correct label
        shell: oc get nodes -l storagenode=glusterfs -o jsonpath='{$.items[?(@.metadata.name=="{{ item }}")].metadata.name}'
        register: node_label_check
        failed_when: node_label_check.stdout != item
        with_items: "{{ new_cns_nodes_internal_fqdn }}"
        changed_when: false

      - name: check daemonset is scaled to 6 nodes
        shell: oc get daemonset -o jsonpath='{$.items[?(@.metadata.name=="glusterfs")].status.desiredNumberScheduled}'
        register: check_daemonset_desired
        failed_when:
          - check_daemonset_desired.stdout != "6"
        changed_when: false

      - name: check daemonset health
        shell: oc get daemonset -o jsonpath='{$.items[?(@.metadata.name=="glusterfs")].status.numberReady}'
        register: check_daemonset_ready
        failed_when:
          - check_daemonset_ready.stdout == ""
          - check_daemonset_ready.stdout|int < check_daemonset_desired.stdout|int
        changed_when: false

      - name: get second cluster-id from heketi
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} cluster list --json
        register: heketi_cluster_list

      - set_fact:
          heketi_clusters: "{{heketi_cluster_list.stdout|from_json}}"

      - set_fact:
          clusterid2: "{{heketi_clusters.clusters|difference([clusterid1])|first}}"

      - name: get nodes from heketi using CNS cluster id 2
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} cluster info {{ clusterid2 }} --json
        register: get_cluster2_nodes
        changed_when: false

      - set_fact:
          cluster2_nodes: "{{ (get_cluster2_nodes.stdout|from_json).nodes }}"

      - name: get IPs of the nodes in the second CNS cluster
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ item }} --json
        with_items: "{{ cluster2_nodes }}"
        register: get_cluster2_node_infos
        changed_when: false

      - name: check second set of cns nodes are pods in second cns cluster
        shell: oc get pods -o wide -n container-native-storage -o jsonpath='{$.items[?(@.status.hostIP=="{{ (item.stdout|from_json).hostnames.storage|first }}")].metadata.name}'
        register: get_cluster2_node
        with_items: "{{ get_cluster2_node_infos.results }}"
        failed_when: get_cluster2_node.stdout == ""
        changed_when: false

      - name: get second storage class
        shell: oc get storageclass/{{ cns_storageclass2 }} -o json
        register: get_storageclass2
        changed_when: false

      - set_fact:
          storageclass2: "{{ get_storageclass2.stdout|from_json }}"

      - name: check second storage class carries second clusters id
        fail:
          msg: "cluster id {{ clusterid2 }} is not set in storageclass {{ cns_storageclass2 }}"
        when: storageclass2.parameters.clusterid != clusterid2

      - name: check presence of additional devices in nodes
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ item }}
        with_items: "{{ cluster2_nodes }}"
        register: cluster2_node
        failed_when: node_brick_device2 not in cluster2_node.stdout
        changed_when: false

      - name: get heketi ID of first node of second CNS cluster
        shell: "heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} topology info | grep -B4 {{ node4_internal_fqdn }} | awk 'match($0, /Node Id: ([a-z0-9].*)/, result) { print result[1] }'"
        register: heketi_node_4_query
        changed_when: false

      - set_fact:
          heketi_node_4_id: "{{ heketi_node_4_query.stdout }}"

      - name: check that first device has been removed of first node of second cns cluster
        shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ heketi_node_4_id }}
        register: cluster2_node4
        failed_when: node_brick_device in cluster2_node4.stdout
        changed_when: false

      - name: log back in as cluster admin into default namespace
        shell: oc login -u system:admin -n default

      - name: check registry pvc from CNS
        shell: oc get pvc/registry-storage -o json
        register: get_registry_pvc
        changed_when: false

      - set_fact:
          registry_pvc: "{{ get_registry_pvc.stdout|from_json }}"

      - name: ensure registry pv comes from registry pvc
        shell: oc get dc/docker-registry -o jsonpath='{$.spec.template.spec.volumes[0].persistentVolumeClaim.claimName}'
        register: get_registry_pvc_name
        failed_when: get_registry_pvc_name.stdout != registry_pvc.metadata.name
        changed_when: false

...
